# Made in the Midlands KTP code

This project contains the scripts and functions produced by David McDonald for the Made in the Midlands (MIM) KTP project. 

## Project structure

In this section, I will explain the purpose of the files in each directory. Some files will be omitted since they are incomplete ideas. 

 - data/: some useful data files.
    * class_to_sector.{csv,json}: map from UK class (SIC code) to group, division and sector. 
    * Manufacturing_SIC_codes.{csv,xlsx}: list of SIC codes in the manufacturing sector (Section C: http://resources.companieshouse.gov.uk/sic/).
    * SIC_hierarchy.json: hierarchy of sector>division>group>class extracted from file data/sic2007summaryofstructurtcm6.csv.
    * SIC_codes_present_in_hierarchy.txt: list of SIC codes that exist in the class->sector hierarchy described in SIC_hierarchy.json. 

 - graph/: all the code for populating a running ArangoDB instance using .csv files. 
    * get_data_for_graph: a batch of queries to pull the relevant data from the MIM datbase into .csv files. 
    * aql_queries.py: a set of AQL queries written as strings to query a running ArangoDB instance. 
    * arango_utils.py: a set of utility functions for Arango in python. 
    * populate_graph.py: a batch of functions to populate an empty ArangoDB database using files generated from the queries in utils/queries.py.

 - prospects/: all the code for filtering the list of prospects from Companies House. 
    * get_prospects_from_companies_house_data.py: a script to filter through a dump of currently registered companies (available here: http://download.companieshouse.gov.uk/en_output.html). The file is assumed to be in the location data/BasicCompanyDataAsOneFile-2021-02-01.csv.gz.
    * filter_prospects.py: applies additional filtering on the prospect businesses. 
    * filter_by_website.py: searches Google for the company name and stores the first result as a simple filter to remove businesses without a (prolific) website. 

 - scrapers/: all of the webscrapers.
    * companies_house/companies_house_scraper.py: will iterate over the files generated by graph/get_data_for_graph.py.
    * company_check/company_check_scraper.py: a scraper utilising Beautiful soup to pull finanical information and directors for a set of businesses. 
    * endole/endole_scraper_selenium.py: a scrpaer utilising selenium to pull finanical/director/competitor data about a set of businesses. You will need to register for a free Endole account and save your credentials into the .env file for it to work. 
    * search_company_website_using_google.py: performs a simple Google search of a company name and returns the top result (can be amended to return any number of results). 

 - utils/: a collection of utility functions. 
    * geo.py: dictionaries used to map postcode to tenancy. 
    * io.py: some util functions to read/write JSON.
    * mysql_utils.py: library of MySQL connector helper functions (to connect/query/update etc.).
    * queries.py: a library of MySQL queries to pull records from the current MIM database. 
    * scraping_utils.py: some BeautifulSoup-related helper functions.
    * selenium_utils.py: some Selenium helper functions.
    * string_matching.py: a matching function to match strings using fuzzywuzzy. 
    * user_utils.py: a quick helper function to partially fill out `company_role` field using `company_position`. 

 - base/: a dump from the Zendesk CRM system (https://www.zendesk.co.uk/) taken on 2021-02-24.

 - ArangoDB_collections.pdf: provides a brief description of the roles of each collection in the ArangoDB graph. 
 - made_production.jpg: a generated entity-relationship diagram for the current MIM database. 
 - MIM_relational_database.pdf: brief summary of all of the tables in the current database.  
 - MIM_database_report.png: a document describing the database and cload hosting options available with a recommendation (Arango on AWS at the end). 

## Requirements
This code was written in Python 3.7.9. Requirements are listed in requirements.txt. Run

```bash
pip install -r requirements.txt
```
to install them.

This code is written to take environment variables from a .env file. You can use .envexample as a starting point, just be sure to rename it to .env

## Connecting to the made_production database
The MIM platform is hosted on Rackspace and uses an internal MySQL database that is not accessible publicly. To connect to it, you will need to use SSH tunnelling. For example:

```bash
ssh -L 3307:${MYSQL_HOST}:${MYSQL_PORT} ${RACKSPACE_USER}@${RACKSPACE_HOST}
```
will set up a tunnel to forward the external port ${MYSQL_PORT} to local port 3307. (The values of ${MYSQL_HOST}, ${MYSQL_PORT}, ${RACKSPACE_USER}, and ${RACKSPACE_HOST} will be provided by Kalexiko). 

After you have set up this tunnel, you can connect to the database with:

```bash
mysql -h 127.0.0.1 -P 3307 -u ${MYSQL_USER} -p made_production
```
where ${MYSQL_USER} will also be provided by Kalexiko.


## Questions
If you have any questions, please email me on davemcdonald93@gmail.com :)

